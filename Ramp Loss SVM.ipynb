{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "import cplex\n",
    "from cplex.exceptions import CplexError\n",
    "import sys\n",
    "from collections import Iterable\n",
    "import pandas as pd\n",
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/liver.txt'\n",
    "\n",
    "def ramp_loss(file_path, C = 10):\n",
    "    \n",
    "    set_kernel = 'linear'\n",
    "    set_C = C\n",
    "    set_dual = False\n",
    "    set_localimplied = 3\n",
    "    set_timelimit = 10\n",
    "    \n",
    "    f = open(file_path, 'r')\n",
    "    data, meta = arff.loadarff(f)\n",
    "    data = np.apply_along_axis(lambda x: x.tolist(), 0, data)\n",
    "\n",
    "    dim = data.shape[1]\n",
    "    X = data[:, 0:dim-2].astype(float)\n",
    "    y = data[:, dim-1].astype(int)\n",
    "    # Process data\n",
    "    processLabel(y)\n",
    "    X = normalize(X)\n",
    "    X = np.nan_to_num(X)\n",
    "    # Split to train and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1238)\n",
    "    \n",
    "    SVM_R(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processLabel(y_set):\n",
    "    for i in range(y_set.shape[0]):\n",
    "        if y_set[i] == 2:\n",
    "            y_set[i] = -1\n",
    "\n",
    "def processData(X_set):\n",
    "    for i in range(X_set.shape[0]):\n",
    "        for j in range(X_set.shape[1]):\n",
    "            X_set[i, j] = (X_set[i, j] - np.mean(X[j])) / 2 * np.std(X[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For square matrices\n",
    "def matrixToList(mat):\n",
    "    matList = []\n",
    "    for i in range(mat.shape[1]):\n",
    "        matList.append([range(mat.shape[1]), mat[i].tolist()])\n",
    "    return matList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(iterable):\n",
    "    for el in iterable:\n",
    "        if isinstance(el, Iterable) and not isinstance(el, str): \n",
    "            yield from flatten(el)\n",
    "        else:\n",
    "            yield el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setProblemData(p, X_train, y_train, dual = False, C = 10):\n",
    "    if dual == False:\n",
    "        print(\"setting primal problem\")\n",
    "        p.set_problem_name(\"SVM_HM_IP1\")\n",
    "        p.objective.set_sense(p.objective.sense.minimize)\n",
    "        # define variable names\n",
    "        \n",
    "        my_colnames = [[\"w\" + str(i) for i in range(1, X_train.shape[1] + 1)], [\"b\"],\n",
    "                       [\"E\" + str(i) for i in range(1, X_train.shape[0] + 1)],\n",
    "                       [\"z\" + str(i) for i in range(1, X_train.shape[0] + 1)]]\n",
    "        # add w(i) variables\n",
    "        p.variables.add(types = [p.variables.type.continuous] * len(my_colnames[0]),\n",
    "                        names = my_colnames[0], lb=[- cplex.infinity]*len(my_colnames[0]))\n",
    "        \n",
    "        qmat = matrixToList(np.identity(X_train.shape[1]))\n",
    "        \n",
    "        p.objective.set_quadratic(qmat)\n",
    "        # add b\n",
    "        p.variables.add(obj = [0], types = p.variables.type.continuous, names = \"b\", lb=[- cplex.infinity])\n",
    "        # add z(i) variables\n",
    "        p.variables.add(obj = [2*C] * len(my_colnames[3]), types = [p.variables.type.binary] * len(my_colnames[3]), \n",
    "                        names = my_colnames[3])\n",
    "        # add E(i)\n",
    "        p.variables.add(obj=[C] * len(my_colnames[2]), \n",
    "                        types = [p.variables.type.continuous] * len(my_colnames[2]), \n",
    "                        names = my_colnames[2],lb=[0] * len(my_colnames[2]), \n",
    "                        ub=[2] * len(my_colnames[2]))\n",
    "        \n",
    "        coefs = []\n",
    "        for i in range(X_train.shape[0]):\n",
    "            coefs.append([y_train[i] * X_train[i], float(y_train[i]), 1.0])\n",
    "            coefs[i][0] = coefs[i][0].tolist()\n",
    "        \n",
    "        wlist = my_colnames[0]\n",
    "        Elist = my_colnames[2]\n",
    "        \n",
    "        \n",
    "        for n in range(X_train.shape[0]):\n",
    "            inds = list(flatten([wlist, \"b\", Elist[n]]))\n",
    "            fcoefs = list(flatten(coefs[n]))\n",
    "            lin_exp = cplex.SparsePair(ind=inds, val=fcoefs)\n",
    "            p.indicator_constraints.add(indvar= my_colnames[3][n], complemented=1,\n",
    "                                        rhs=1.0, sense='G',\n",
    "                                        lin_expr=lin_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(p, X_train, y_train, X_test, y_test, Dual=False):\n",
    "    \n",
    "    #X_test, y_test\n",
    "    Test_set = X_test\n",
    "    label_test = y_test\n",
    "   \n",
    "    sol = p.solution\n",
    "    global test_predicted\n",
    "    test_predicted = np.zeros(shape=label_test.shape[0])\n",
    "    \n",
    "    if Dual == False:\n",
    "        sol_vals = []\n",
    "        \n",
    "        for i in range(X_train.shape[1] + 1):\n",
    "            sol_vals.append(sol.get_values(i))\n",
    "        \n",
    "        w = np.asarray(sol_vals[0:len(sol_vals)-1])\n",
    "        b = sol_vals[len(sol_vals)-1]\n",
    "        \n",
    "        for j in range(Test_set.shape[0]):\n",
    "            test_predicted[j] = np.sign(np.inner(w, X_test[j]) + b)\n",
    "    \n",
    "    TP = np.zeros(shape=label_test.shape[0])\n",
    "    TN = np.zeros(shape=label_test.shape[0])\n",
    "    FP = np.zeros(shape=label_test.shape[0])\n",
    "    FN = np.zeros(shape=label_test.shape[0])\n",
    "    \n",
    "    for i in range(label_test.shape[0]):\n",
    "        if label_test[i] == 1 and test_predicted[i] == 1:\n",
    "            TP[i] = 1\n",
    "        elif label_test[i] == 1 and test_predicted[i] == -1:\n",
    "            FN[i] = 1\n",
    "        elif label_test[i] == -1 and test_predicted[i] == 1:    \n",
    "            FP[i] = 1\n",
    "        elif label_test[i] == -1 and test_predicted[i] == -1:\n",
    "            TN[i] = 1\n",
    "    \n",
    "    Confusion_matrix = [[np.sum(TP), np.sum(FN)], [np.sum(FP), np.sum(TN)]]        \n",
    "    print(\"Confusion matrix = ([TP, FN], [FP, TN]) = \", Confusion_matrix)\n",
    "\n",
    "\n",
    "    Sensitivity = Confusion_matrix[0][0] / (Confusion_matrix[0][0] + Confusion_matrix[0][1])\n",
    "    Precision = Confusion_matrix[0][0] / (Confusion_matrix[0][0] + Confusion_matrix[1][0])\n",
    "    Accuracy = (Confusion_matrix[0][0] + Confusion_matrix[1][1]) / (Confusion_matrix[0][0] + \n",
    "                Confusion_matrix[1][1] + Confusion_matrix[0][1] + Confusion_matrix[1][0])   \n",
    "    print(\"Classifier Accuracy = \", Accuracy )\n",
    "    print(\"Precision = \", Precision)\n",
    "    print(\"Sensitivity = \", Sensitivity)\n",
    "           \n",
    "    return test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_R(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    set_localimplied = 3\n",
    "    set_timelimit = 10\n",
    "    \n",
    "    p = cplex.Cplex()\n",
    "    setProblemData(p, X_train, y_train)\n",
    "    \n",
    "    p.parameters.mip.cuts.localimplied.set(set_localimplied)\n",
    "    p.parameters.timelimit.set(set_timelimit)\n",
    "    \n",
    "    print(\"Solving Ramp Loss SVM primal problem\")\n",
    "    \n",
    "    p.solve()\n",
    "    \n",
    "    #p.write(\"SVMIP1_HM.lp\")\n",
    "\n",
    "    sol = p.solution\n",
    "   \n",
    "    #sol.write(\"Primal_Solution.lp\")\n",
    "\n",
    "    # solution.get_status() returns an integer code\n",
    "    print(\"Solution status = \", sol.get_status(), \":\", end=' ')\n",
    "    \n",
    "    # the following line prints the corresponding string\n",
    "    print(sol.status[sol.get_status()])\n",
    "    print(\"Solution value  = \", sol.get_objective_value())\n",
    "\n",
    "    numcols = p.variables.get_num()\n",
    "\n",
    "    for j in range(numcols):\n",
    "        if sol.get_values(j) != 0:\n",
    "            print(\"Column %d: Value = %10f\" % (j, sol.get_values(j)))\n",
    "    \n",
    "    predict(p, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting primal problem\n",
      "Solving Ramp Loss SVM primal problem\n",
      "CPXPARAM_TimeLimit                               10\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "CPXPARAM_MIP_Cuts_LocalImplied                   3\n",
      "Tried aggregator 1 time.\n",
      "Reduced MIQP has 207 rows, 627 columns, and 1656 nonzeros.\n",
      "Reduced MIQP has 207 binaries, 0 generals, 0 SOSs, and 207 indicators.\n",
      "Reduced MIQP objective Q matrix has 5 nonzeros.\n",
      "Presolve time = 0.07 sec. (0.68 ticks)\n",
      "Probing time = 0.00 sec. (0.04 ticks)\n",
      "Tried aggregator 1 time.\n",
      "Reduced MIQP has 207 rows, 627 columns, and 1656 nonzeros.\n",
      "Reduced MIQP has 207 binaries, 0 generals, 0 SOSs, and 207 indicators.\n",
      "Reduced MIQP objective Q matrix has 5 nonzeros.\n",
      "Presolve time = 0.02 sec. (0.43 ticks)\n",
      "Probing time = 0.00 sec. (0.04 ticks)\n",
      "MIP emphasis: balance optimality and feasibility.\n",
      "MIP search method: dynamic search.\n",
      "Parallel mode: none, using 1 thread.\n",
      "Root relaxation solution time = 0.00 sec. (0.62 ticks)\n",
      "\n",
      "        Nodes                                         Cuts/\n",
      "   Node  Left     Objective  IInf  Best Integer    Best Bound    ItCnt     Gap\n",
      "\n",
      "      0     0        0.0000   207                      0.0000        0         \n",
      "*     0+    0                         1693.7943        0.0000           100.00%\n",
      "      0     2        0.0000   207     1693.7943        0.0000        0  100.00%\n",
      "Elapsed time = 0.45 sec. (30.93 ticks, tree = 0.01 MB, solutions = 1)\n",
      "    200   197      220.0000    75     1693.7943       20.0000      279   98.82%\n",
      "    450   443      513.6718   139     1693.7943       36.9900     1332   97.82%\n",
      "    548   514      675.2960   140     1693.7943       36.9900     1912   97.82%\n",
      "    678   582      129.0615   144     1693.7943       36.9900     2799   97.82%\n",
      "    857   681     1653.8091    21     1693.7943       36.9900     3610   97.82%\n",
      "    948   732      699.9607   133     1693.7943       39.8821     4119   97.65%\n",
      "   1088   820      306.3114   176     1693.7943       40.0000     4986   97.64%\n",
      "   1248   905       58.7062   146     1693.7943       40.0000     5945   97.64%\n",
      "   1366   973     1154.0916    95     1693.7943       40.0000     6623   97.64%\n",
      "   1927  1430     1381.6000    69     1693.7943       54.8233    10449   96.76%\n",
      "Elapsed time = 6.83 sec. (3178.31 ticks, tree = 0.55 MB, solutions = 1)\n",
      "   2546  2037      539.4186   153     1693.7943       57.4224    14502   96.61%\n",
      "*  2878+ 2352                         1691.2438       59.0534            96.51%\n",
      "\n",
      "Root node processing (before b&c):\n",
      "  Real time             =    0.43 sec. (30.95 ticks)\n",
      "Sequential b&c:\n",
      "  Real time             =    9.57 sec. (4841.07 ticks)\n",
      "                          ------------\n",
      "Total (root+branch&cut) =   10.00 sec. (4872.02 ticks)\n",
      "Solution status =  107 : MIP_time_limit_feasible\n",
      "Solution value  =  1691.2437911191519\n",
      "Column 0: Value =  -1.370330\n",
      "Column 1: Value =  -0.410978\n",
      "Column 2: Value =   2.338583\n",
      "Column 3: Value =  -6.800012\n",
      "Column 4: Value =  -1.938190\n",
      "Column 5: Value =   1.834338\n",
      "Column 52: Value =   1.000000\n",
      "Column 110: Value =   1.000000\n",
      "Column 213: Value =   0.304000\n",
      "Column 214: Value =   1.689358\n",
      "Column 215: Value =   1.951721\n",
      "Column 216: Value =   0.070444\n",
      "Column 217: Value =   0.917938\n",
      "Column 218: Value =   1.666116\n",
      "Column 219: Value =   1.698017\n",
      "Column 220: Value =   1.736590\n",
      "Column 221: Value =   1.118938\n",
      "Column 222: Value =   0.552514\n",
      "Column 223: Value =   0.468386\n",
      "Column 224: Value =   1.679747\n",
      "Column 225: Value =   0.366262\n",
      "Column 226: Value =   1.219388\n",
      "Column 227: Value =   0.025390\n",
      "Column 228: Value =   0.663968\n",
      "Column 229: Value =   1.714974\n",
      "Column 230: Value =   1.267933\n",
      "Column 231: Value =   0.695457\n",
      "Column 232: Value =   1.667210\n",
      "Column 233: Value =   0.437091\n",
      "Column 234: Value =   1.247414\n",
      "Column 235: Value =   0.999583\n",
      "Column 236: Value =   1.260669\n",
      "Column 237: Value =   0.522594\n",
      "Column 238: Value =   0.056385\n",
      "Column 239: Value =   0.047334\n",
      "Column 240: Value =   1.448236\n",
      "Column 241: Value =   1.502360\n",
      "Column 242: Value =   0.267622\n",
      "Column 243: Value =   0.186261\n",
      "Column 244: Value =   0.490427\n",
      "Column 245: Value =   0.456190\n",
      "Column 246: Value =   0.485955\n",
      "Column 247: Value =   0.320771\n",
      "Column 248: Value =   0.551037\n",
      "Column 249: Value =   1.357241\n",
      "Column 250: Value =   0.331473\n",
      "Column 251: Value =   0.441538\n",
      "Column 252: Value =   1.701156\n",
      "Column 253: Value =   0.881165\n",
      "Column 254: Value =   1.284712\n",
      "Column 256: Value =   1.704617\n",
      "Column 257: Value =   0.604100\n",
      "Column 261: Value =   1.456378\n",
      "Column 262: Value =   1.599963\n",
      "Column 263: Value =   0.746725\n",
      "Column 264: Value =   1.488698\n",
      "Column 265: Value =   0.354177\n",
      "Column 266: Value =   0.384762\n",
      "Column 267: Value =   0.136416\n",
      "Column 268: Value =   0.438985\n",
      "Column 269: Value =   0.167117\n",
      "Column 270: Value =   0.390469\n",
      "Column 271: Value =   0.265732\n",
      "Column 272: Value =   0.453012\n",
      "Column 273: Value =   0.118992\n",
      "Column 274: Value =   0.370575\n",
      "Column 275: Value =   0.059377\n",
      "Column 276: Value =   0.194743\n",
      "Column 277: Value =   0.552539\n",
      "Column 278: Value =   1.563405\n",
      "Column 279: Value =   0.292612\n",
      "Column 281: Value =   0.301225\n",
      "Column 283: Value =   0.431203\n",
      "Column 284: Value =   0.495399\n",
      "Column 285: Value =   0.178489\n",
      "Column 286: Value =   0.401478\n",
      "Column 287: Value =   2.000000\n",
      "Column 288: Value =   0.964858\n",
      "Column 289: Value =   0.245541\n",
      "Column 290: Value =   1.222064\n",
      "Column 291: Value =   1.600144\n",
      "Column 292: Value =   1.950687\n",
      "Column 293: Value =   2.000000\n",
      "Column 294: Value =   1.056090\n",
      "Column 295: Value =   0.434896\n",
      "Column 296: Value =   0.033757\n",
      "Column 297: Value =   0.137573\n",
      "Column 298: Value =   1.503484\n",
      "Column 299: Value =   0.790106\n",
      "Column 300: Value =   0.482437\n",
      "Column 301: Value =   1.087588\n",
      "Column 302: Value =   0.261552\n",
      "Column 303: Value =   0.310541\n",
      "Column 304: Value =   1.075652\n",
      "Column 305: Value =   1.014950\n",
      "Column 306: Value =   0.329356\n",
      "Column 307: Value =   0.113006\n",
      "Column 309: Value =   1.336841\n",
      "Column 310: Value =   0.250935\n",
      "Column 311: Value =   1.624444\n",
      "Column 312: Value =   0.383296\n",
      "Column 313: Value =   0.194422\n",
      "Column 314: Value =   0.499226\n",
      "Column 315: Value =   1.138623\n",
      "Column 316: Value =   1.039785\n",
      "Column 318: Value =   0.256350\n",
      "Column 320: Value =   0.235494\n",
      "Column 322: Value =   0.023691\n",
      "Column 323: Value =   0.844343\n",
      "Column 324: Value =   0.046321\n",
      "Column 325: Value =   1.704617\n",
      "Column 326: Value =   0.552064\n",
      "Column 327: Value =   1.339389\n",
      "Column 328: Value =   0.446542\n",
      "Column 329: Value =   2.000000\n",
      "Column 330: Value =   0.164175\n",
      "Column 331: Value =   0.809804\n",
      "Column 332: Value =   1.062346\n",
      "Column 333: Value =   0.423228\n",
      "Column 334: Value =   0.674256\n",
      "Column 335: Value =   0.530702\n",
      "Column 336: Value =   0.226369\n",
      "Column 337: Value =   1.743467\n",
      "Column 338: Value =   0.482096\n",
      "Column 339: Value =   0.914049\n",
      "Column 340: Value =   1.292314\n",
      "Column 341: Value =   1.602042\n",
      "Column 342: Value =   0.627445\n",
      "Column 343: Value =   1.567637\n",
      "Column 344: Value =   0.094352\n",
      "Column 345: Value =   1.688786\n",
      "Column 348: Value =   0.266199\n",
      "Column 349: Value =   1.716739\n",
      "Column 350: Value =   1.280434\n",
      "Column 351: Value =   0.074063\n",
      "Column 352: Value =   1.548433\n",
      "Column 353: Value =   0.996410\n",
      "Column 354: Value =   1.275204\n",
      "Column 355: Value =   1.267167\n",
      "Column 356: Value =   1.856126\n",
      "Column 357: Value =   0.882661\n",
      "Column 358: Value =   1.633775\n",
      "Column 359: Value =   0.129373\n",
      "Column 360: Value =   0.270075\n",
      "Column 362: Value =   1.793150\n",
      "Column 363: Value =   1.795113\n",
      "Column 364: Value =   1.734164\n",
      "Column 365: Value =   1.577358\n",
      "Column 366: Value =   0.272137\n",
      "Column 367: Value =   1.281386\n",
      "Column 368: Value =   1.689008\n",
      "Column 369: Value =   1.973116\n",
      "Column 370: Value =   0.657535\n",
      "Column 371: Value =   0.701165\n",
      "Column 375: Value =   1.190861\n",
      "Column 376: Value =   0.420152\n",
      "Column 377: Value =   0.621033\n",
      "Column 378: Value =   1.436687\n",
      "Column 379: Value =   1.486450\n",
      "Column 380: Value =   1.149505\n",
      "Column 381: Value =   0.657621\n",
      "Column 382: Value =   0.327485\n",
      "Column 383: Value =   1.453216\n",
      "Column 384: Value =   0.055599\n",
      "Column 385: Value =   0.636384\n",
      "Column 386: Value =   1.771172\n",
      "Column 387: Value =   0.944795\n",
      "Column 389: Value =   0.286368\n",
      "Column 390: Value =   0.402381\n",
      "Column 391: Value =   1.324673\n",
      "Column 392: Value =   1.499486\n",
      "Column 393: Value =   2.000000\n",
      "Column 394: Value =   0.207176\n",
      "Column 395: Value =   1.309567\n",
      "Column 397: Value =   0.504027\n",
      "Column 398: Value =   0.680433\n",
      "Column 399: Value =   1.500819\n",
      "Column 400: Value =   0.545294\n",
      "Column 401: Value =   0.154904\n",
      "Column 403: Value =   1.017819\n",
      "Column 404: Value =   0.944783\n",
      "Column 405: Value =   0.661538\n",
      "Column 406: Value =   0.240771\n",
      "Column 408: Value =   1.420902\n",
      "Column 409: Value =   0.311479\n",
      "Column 410: Value =   0.284274\n",
      "Column 411: Value =   0.551328\n",
      "Column 412: Value =   0.209152\n",
      "Column 413: Value =   0.027754\n",
      "Column 414: Value =   2.000000\n",
      "Column 415: Value =   1.507351\n",
      "Column 416: Value =   1.547667\n",
      "Column 417: Value =   1.340706\n",
      "Column 418: Value =   0.514221\n",
      "Column 419: Value =   1.095648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix = ([TP, FN], [FP, TN]) =  [[8.0, 51.0], [0.0, 79.0]]\n",
      "Classifier Accuracy =  0.6304347826086957\n",
      "Precision =  1.0\n",
      "Sensitivity =  0.13559322033898305\n"
     ]
    }
   ],
   "source": [
    "ramp_loss(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
